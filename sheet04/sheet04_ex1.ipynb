{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5\n",
    "initial_state = np.array([0,0])\n",
    "loc_a = np.array([0,1])\n",
    "loc_a_ = np.array([4,1])\n",
    "rew_a = 10\n",
    "loc_b = np.array([0,3])\n",
    "loc_b_ = np.array([2,3])\n",
    "rew_b = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(state, action):\n",
    "    if state[0] == loc_a[0] and state[1] == loc_a[1]:\n",
    "                    return loc_a_, rew_a\n",
    "    elif state[0] == loc_b[0] and state[1] == loc_b[1]:\n",
    "                    return loc_b_, rew_b\n",
    "    else:\n",
    "        if action == 0: # North\n",
    "            if state[0] == 0:\n",
    "                return state, -1\n",
    "            else:\n",
    "                state = state + np.array([-1,0])\n",
    "                return state, 0\n",
    "        if action == 1: #South\n",
    "            if state[0] == grid_size - 1:\n",
    "                return state, -1\n",
    "            else:\n",
    "                state = state + np.array([1,0])\n",
    "                return state, 0\n",
    "        if action == 2: #West\n",
    "            if state[1] == 0:\n",
    "                return state, -1\n",
    "            else:\n",
    "                state = state + np.array([0,-1])\n",
    "                return state, 0\n",
    "        if action == 3: #East\n",
    "            if state[1] == grid_size -1 :\n",
    "                return state, -1\n",
    "            else:\n",
    "                state = state + np.array([0,1])\n",
    "                return state, 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "max_steps = 50   # Max steps per episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice([0,1,2,3]) \n",
    "    else:\n",
    "        best_actions = np.where(Q[state,:] == np.max(Q[state,:]))[0]\n",
    "        return np.random.choice(best_actions)    # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_off_policy(episodes, len_episode):\n",
    "    grid_size = 5\n",
    "    n_actions = 4\n",
    "    \n",
    "    Q = np.zeros((grid_size, grid_size, n_actions))\n",
    "    C = np.zeros((grid_size, grid_size, n_actions))\n",
    "\n",
    "    target_policy = 1/n_actions * np.ones((grid_size, grid_size, n_actions))\n",
    "\n",
    "    total_rewards = np.zeros(episodes)\n",
    "\n",
    "    #Monte-Carlo Prediction\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        state = np.array([2,2])\n",
    "        rewards = np.zeros(len_episode + 1)\n",
    "        states = np.zeros((len_episode + 1, 2))\n",
    "        states[0, :] = state #Save the initial state\n",
    "        actions = []\n",
    "\n",
    "\n",
    "        for iter in range(len_episode):\n",
    "            current_action = epsilon_greedy_policy(state, Q, 1)\n",
    "            new_state, _ = next_state(state, current_action) # reward of this behavior policy not of interest\n",
    "\n",
    "            state = new_state\n",
    "            states[iter+1, :] = state\n",
    "\n",
    "            if current_action == 'N': \n",
    "                current_action = 0\n",
    "            elif current_action == 'S': \n",
    "                current_action = 1\n",
    "            elif current_action == 'W': \n",
    "                current_action = 2\n",
    "            elif current_action == 'E': \n",
    "                current_action = 3\n",
    "\n",
    "            actions.append(current_action)\n",
    "            \n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        total_rewards[episode] = np.sum(rewards)\n",
    "\n",
    "        for t in range(len_episode , 0, -1):\n",
    "            \n",
    "            current_state_x = int(states[t-1, 0])\n",
    "            current_state_y = int(states[t-1, 1])\n",
    "\n",
    "            G = G * gamma + rewards[t] \n",
    "        \n",
    "            C[current_state_x, current_state_y, actions[t-1]] += W\n",
    "            Q[current_state_x, current_state_y, actions[t-1]] += (W / C[current_state_x, current_state_y, actions[t-1]]) * (G - Q[current_state_x, current_state_y, actions[t-1]])\n",
    "\n",
    "            best_actions = np.where(Q[current_state_x, current_state_y, :] == np.max(Q[current_state_x, current_state_y, :]))[0]\n",
    "            \n",
    "            target_policy[current_state_x, current_state_y, :] = 0\n",
    "            target_policy[current_state_x, current_state_y, best_actions] = 1/len(best_actions)\n",
    "\n",
    "            W *= 1/(1/(n_actions))\n",
    "        \n",
    "\n",
    "        rewards_target = np.zeros(len_episode + 1)\n",
    "\n",
    "        # reward of target \n",
    "        state = np.array([2,2])\n",
    "        for i in range(len_episode):\n",
    "            current_action = np.argmax(target_policy[state[0],state[1], :])\n",
    "            new_state, reward = next_state(state, current_action) # reward of this behavior policy not of interest\n",
    "\n",
    "            state = new_state\n",
    "            rewards_target[iter+1] = reward\n",
    "        total_rewards[episode] = np.sum(rewards_target)\n",
    "\n",
    "    return target_policy, Q, total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_action_value_with_numbers(Q):\n",
    "    Q = np.round(Q, 2)\n",
    "    n_rows, n_cols, n_dirs = Q.shape\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(n_cols, n_rows))\n",
    "    ax.set_xlim(-0.5, n_cols - 0.5)\n",
    "    ax.set_ylim(-0.5, n_rows - 0.5)\n",
    "    ax.set_xticks(np.arange(-0.5, n_cols, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, n_rows, 1))\n",
    "    ax.grid(True)\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax.text(j, i - 0.2, f\"{Q[i, j, 0]}\", ha='center', va='center', fontsize=8, color=\"red\")  # North\n",
    "            ax.text(j, i + 0.2, f\"{Q[i, j, 1]}\", ha='center', va='center', fontsize=8, color=\"green\")  # South\n",
    "            ax.text(j - 0.2, i, f\"{Q[i, j, 2]}\", ha='center', va='center', fontsize=8, color=\"blue\")  # West\n",
    "            ax.text(j + 0.2, i, f\"{Q[i, j, 3]}\", ha='center', va='center', fontsize=8, color=\"orange\")  # East\n",
    "\n",
    "    # Reverse the y-axis to align with matrix indexing\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(\"Action Value Visualization\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(policy):\n",
    "    n_rows, n_cols, n_dirs = policy.shape\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(n_cols, n_rows))\n",
    "    ax.set_xlim(-0.5, n_cols - 0.5)\n",
    "    ax.set_ylim(-0.5, n_rows - 0.5)\n",
    "    ax.set_xticks(np.arange(-0.5, n_cols, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, n_rows, 1))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Arrow parameters\n",
    "    arrow_params = {\n",
    "        \"head_width\": 0.2,\n",
    "        \"head_length\": 0.2,\n",
    "        \"length_includes_head\": True,\n",
    "        \"color\": \"blue\",\n",
    "    }\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            # Check each direction\n",
    "            if policy[i, j, 0] > 0:  # North\n",
    "                ax.arrow(j, i, 0, -0.4, **arrow_params)\n",
    "            if policy[i, j, 1] > 0:  # South\n",
    "                ax.arrow(j, i, 0, 0.4, **arrow_params)\n",
    "            if policy[i, j, 2] > 0:  # West\n",
    "                ax.arrow(j, i, -0.4, 0, **arrow_params)\n",
    "            if policy[i, j, 3] > 0:  # East\n",
    "                ax.arrow(j, i, 0.4, 0, **arrow_params)\n",
    "    \n",
    "    # Reverse the y-axis to align with matrix indexing\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(\"Policy Visualization\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_policy, Q, total_target_rewards = mc_off_policy(20000, 50)\n",
    "visualize_policy(target_policy)\n",
    "visualize_action_value_with_numbers(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
