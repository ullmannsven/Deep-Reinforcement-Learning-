{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5\n",
    "initial_state = np.array([0,0])\n",
    "loc_a = np.array([0,1])\n",
    "loc_a_ = np.array([4,1])\n",
    "rew_a = 5\n",
    "loc_b = np.array([0,3])\n",
    "loc_b_ = np.array([4,3])\n",
    "rew_b = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(state, action):\n",
    "    if state[0] == loc_a[0] and state[1] == loc_a[1]:\n",
    "                    return loc_a_, rew_a\n",
    "    elif state[0] == loc_b[0] and state[1] == loc_b[1]:\n",
    "                    return loc_b_, rew_b\n",
    "    else:\n",
    "        if action == 'N':\n",
    "            if state[0] == 0:\n",
    "                return state, -1\n",
    "            else:\n",
    "                state = state + np.array([-1,0])\n",
    "                return state, 0\n",
    "        if action == 'S':\n",
    "            if state[0] == grid_size - 1:\n",
    "                return state, -1\n",
    "            else:\n",
    "                state = state + np.array([1,0])\n",
    "                return state, 0\n",
    "        if action == 'W':\n",
    "            if state[1] == 0:\n",
    "                return state, -1\n",
    "            else:\n",
    "                state = state + np.array([0,-1])\n",
    "                return state, 0\n",
    "        if action == 'E':\n",
    "            if state[1] == grid_size -1 :\n",
    "                return state, -1\n",
    "            else:\n",
    "                state = state + np.array([0,1])\n",
    "                return state, 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "max_steps = 50   # Max steps per episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(['N', 'S', 'W', 'E']) \n",
    "    else:\n",
    "        best_actions = np.where(Q[state,:] == np.max(Q[state,:]))[0]\n",
    "        return np.random.choice(best_actions)    # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_off_policy(episodes, len_episode, epsilon_soft_start, epsilon_decay):\n",
    "    grid_size = 5\n",
    "    n_actions = 4\n",
    "    \n",
    "    Q = np.zeros((grid_size, grid_size, n_actions))\n",
    "    C = np.zeros((grid_size, grid_size, n_actions))\n",
    "\n",
    "    target_policy = 1/n_actions * np.ones((grid_size, grid_size, n_actions))\n",
    "\n",
    "    total_rewards = np.zeros(episodes)\n",
    "\n",
    "    #Monte-Carlo Prediction\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        state = np.array([2,2])\n",
    "        rewards = np.zeros(len_episode + 1)\n",
    "        states = np.zeros((len_episode + 1, 2))\n",
    "        states[0, :] = state #Save the initial state\n",
    "        actions = []\n",
    "\n",
    "        epsilon_soft = max(0.1, epsilon_soft_start * (epsilon_decay ** episode))\n",
    "        print(epsilon_soft)\n",
    "\n",
    "        for iter in range(len_episode):\n",
    "            current_action = epsilon_greedy_policy(state, Q, epsilon_soft)\n",
    "            new_state, reward = next_state(state, current_action)\n",
    "\n",
    "            state = new_state\n",
    "            states[iter+1, :] = state\n",
    "\n",
    "            if current_action == 'N': \n",
    "                current_action = 0\n",
    "            elif current_action == 'S': \n",
    "                current_action = 1\n",
    "            elif current_action == 'W': \n",
    "                current_action = 2\n",
    "            elif current_action == 'E': \n",
    "                current_action = 3\n",
    "\n",
    "            actions.append(current_action)\n",
    "            rewards[iter+1] = reward\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        total_rewards[episode] = np.sum(rewards)\n",
    "\n",
    "        for t in range(len_episode , 0, -1):\n",
    "            \n",
    "            current_state_x = int(states[t-1, 0])\n",
    "            current_state_y = int(states[t-1, 1])\n",
    "\n",
    "            G = G * gamma + rewards[t] \n",
    "        \n",
    "            C[current_state_x, current_state_y, actions[t-1]] += W\n",
    "            Q[current_state_x, current_state_y, actions[t-1]] += (W / C[current_state_x, current_state_y, actions[t-1]]) * (G - Q[current_state_x, current_state_y, actions[t-1]])\n",
    "\n",
    "            best_actions = np.where(Q[current_state_x, current_state_y, :] == np.max(Q[current_state_x, current_state_y, :]))[0]\n",
    "            \n",
    "            target_policy[current_state_x, current_state_y, :] = 0\n",
    "            target_policy[current_state_x, current_state_y, best_actions] = 1/len(best_actions)\n",
    "\n",
    "            W *= 1/(1/(n_actions))\n",
    "        \n",
    "    return target_policy, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_action_value_with_numbers(Q):\n",
    "    Q = np.round(Q, 2)\n",
    "    n_rows, n_cols, n_dirs = Q.shape\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(n_cols, n_rows))\n",
    "    ax.set_xlim(-0.5, n_cols - 0.5)\n",
    "    ax.set_ylim(-0.5, n_rows - 0.5)\n",
    "    ax.set_xticks(np.arange(-0.5, n_cols, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, n_rows, 1))\n",
    "    ax.grid(True)\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax.text(j, i - 0.2, f\"{Q[i, j, 0]}\", ha='center', va='center', fontsize=8, color=\"red\")  # North\n",
    "            ax.text(j, i + 0.2, f\"{Q[i, j, 1]}\", ha='center', va='center', fontsize=8, color=\"green\")  # South\n",
    "            ax.text(j - 0.2, i, f\"{Q[i, j, 2]}\", ha='center', va='center', fontsize=8, color=\"blue\")  # West\n",
    "            ax.text(j + 0.2, i, f\"{Q[i, j, 3]}\", ha='center', va='center', fontsize=8, color=\"orange\")  # East\n",
    "\n",
    "    # Reverse the y-axis to align with matrix indexing\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(\"Action Value Visualization\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(policy):\n",
    "    n_rows, n_cols, n_dirs = policy.shape\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(n_cols, n_rows))\n",
    "    ax.set_xlim(-0.5, n_cols - 0.5)\n",
    "    ax.set_ylim(-0.5, n_rows - 0.5)\n",
    "    ax.set_xticks(np.arange(-0.5, n_cols, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, n_rows, 1))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Arrow parameters\n",
    "    arrow_params = {\n",
    "        \"head_width\": 0.2,\n",
    "        \"head_length\": 0.2,\n",
    "        \"length_includes_head\": True,\n",
    "        \"color\": \"blue\",\n",
    "    }\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            # Check each direction\n",
    "            if policy[i, j, 0] > 0:  # North\n",
    "                ax.arrow(j, i, 0, -0.4, **arrow_params)\n",
    "            if policy[i, j, 1] > 0:  # South\n",
    "                ax.arrow(j, i, 0, 0.4, **arrow_params)\n",
    "            if policy[i, j, 2] > 0:  # West\n",
    "                ax.arrow(j, i, -0.4, 0, **arrow_params)\n",
    "            if policy[i, j, 3] > 0:  # East\n",
    "                ax.arrow(j, i, 0.4, 0, **arrow_params)\n",
    "    \n",
    "    # Reverse the y-axis to align with matrix indexing\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(\"Policy Visualization\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m target_policy, Q \u001b[38;5;241m=\u001b[39m \u001b[43mmc_off_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m visualize_policy(target_policy)\n\u001b[0;32m      3\u001b[0m visualize_action_value_with_numbers(Q)\n",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m, in \u001b[0;36mmc_off_policy\u001b[1;34m(episodes, len_episode, epsilon_soft_start, epsilon_decay)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(len_episode):\n\u001b[0;32m     24\u001b[0m     current_action \u001b[38;5;241m=\u001b[39m epsilon_greedy_policy(state, Q, epsilon_soft)\n\u001b[1;32m---> 25\u001b[0m     new_state, reward \u001b[38;5;241m=\u001b[39m next_state(state, current_action)\n\u001b[0;32m     27\u001b[0m     state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[0;32m     28\u001b[0m     states[\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m=\u001b[39m state\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "target_policy, Q = mc_off_policy(5000, 50, 1, 0.99)\n",
    "visualize_policy(target_policy)\n",
    "visualize_action_value_with_numbers(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
